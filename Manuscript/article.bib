@article{Albert:1993,
author = { James H.   Albert  and  Siddhartha   Chib },
title = {Bayesian Analysis of Binary and Polychotomous Response Data},
journal = {Journal of the American Statistical Association},
volume = {88},
number = {422},
pages = {669-679},
year  = {1993},
publisher = {Taylor & Francis},
doi = {10.1080/01621459.1993.10476321},

URL = { 
        https://www.tandfonline.com/doi/abs/10.1080/01621459.1993.10476321
    
},
eprint = { 
        https://www.tandfonline.com/doi/pdf/10.1080/01621459.1993.10476321
    
}
}

@article {Berkel:2020,
      author = "Kees van Berkel and Suzanne van der Doef and Barry Schouten",
      title = "Implementing Adaptive Survey Design with an Application to the Dutch Health Survey",
      journal = "Journal of Official Statistics",
      year = "2020",
      publisher = "Sciendo",
      address = "Berlin",
      volume = "36",
      number = "3",
      doi = "https://doi.org/10.2478/jos-2020-0031",
      pages=      "609-629",
}

@article{Bethlehem:1988,
  title={Reduction of nonresponse bias through regression estimation},
  author={Bethlehem, Jelke G},
  journal={Journal of Official Statistics},
  volume={4},
  number={3},
  pages={251},
  year={1988},
  publisher={Statistics Sweden (SCB)}
}

@book{Bethlehem:2011,
  title={Handbook of Nonresponse in Household Surveys},
  author={Bethlehem, J. and Cobben, F. and Schouten, B.},
  isbn={9781118102220},
  series={Wiley Handbooks in Survey Methodology},
  url={},
  year={2011},
  publisher={John Wiley \& Sons},
  address={Hoboken, NJ}
}


@article{Burger:2017,
      author = "Joep Burger and Koen Perryck and Barry Schouten",
      title = "Robustness of Adaptive Survey Designs to Inaccuracy of Design Parameters",
      journal = "Journal of Official Statistics",
      year = "2017",
      publisher = "Sciendo",
      address = "Berlin",
      volume = "33",
      number = "3",
      doi = "https://doi.org/10.1515/jos-2017-0032",
      pages=      "687-708",
}

@article{Spiegelhalter:2002,
author = {Spiegelhalter, David J. and Best, Nicola G. and Carlin, Bradley P. and Van Der Linde, Angelika},
title = {Bayesian measures of model complexity and fit},
journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
volume = {64},
number = {4},
pages = {583-639},
keywords = {Bayesian model comparison, Decision theory, Deviance information criterion, Effective number of parameters, Hierarchical models, Information theory, Leverage, Markov chain Monte Carlo methods, Model dimension},
doi = {https://doi.org/10.1111/1467-9868.00353},
url = {},
eprint = {https://rss.onlinelibrary.wiley.com/doi/pdf/10.1111/1467-9868.00353},
abstract = {Summary. We consider the problem of comparing complex hierarchical models in which the number of parameters is not clearly defined. Using an information theoretic argument we derive a measure pD for the effective number of parameters in a model as the difference between the posterior mean of the deviance and the deviance at the posterior means of the parameters of interest. In general pD approximately corresponds to the trace of the product of Fisher's information and the posterior covariance, which in normal models is the trace of the ‘hat’ matrix projecting observations onto fitted values. Its properties in exponential families are explored. The posterior mean deviance is suggested as a Bayesian measure of fit or adequacy, and the contributions of individual observations to the fit and complexity can give rise to a diagnostic plot of deviance residuals against leverages. Adding pD to the posterior mean deviance gives a deviance information criterion for comparing models, which is related to other information criteria and has an approximate decision theoretic justification. The procedure is illustrated in some examples, and comparisons are drawn with alternative Bayesian and classical proposals. Throughout it is emphasized that the quantities required are trivial to compute in a Markov chain Monte Carlo analysis.},
year = {2002}
}


@book{Gelman:2013,
  title={Bayesian Data Analysis, Third Edition},
  author={Gelman, A. and Carlin, J.B. and Stern, H.S. and Dunson, D.B. and Vehtari, A. and Rubin, D.B.},
  isbn={9781439840955},
  lccn={2013039507},
  series={Chapman \& Hall/CRC Texts in Statistical Science},
  url={},
  year={2013},
  publisher={CRC Press},
  address={Boca Raton, FL}
}

@article{Gelman:2019,
author = {Andrew Gelman and Ben Goodrich and Jonah Gabry and Aki Vehtari},
title = {R-squared for Bayesian Regression Models},
journal = {The American Statistician},
volume = {73},
number = {3},
pages = {307-309},
year  = {2019},
publisher = {Taylor & Francis},
doi = {10.1080/00031305.2018.1549100},

URL = {},
eprint = { 
        https://doi.org/10.1080/00031305.2018.1549100
    
}

}

@article{Groves:2000,
    author = {Groves, Robert M and Singer, Eleanor and Corning, Amy},
    title = "{Leverage-Saliency Theory of Survey Participation: Description and an Illustration}",
    journal = {Public Opinion Quarterly},
    volume = {64},
    number = {3},
    pages = {299-308},
    year = {2000},
    month = {11},
    issn = {0033-362X},
    doi = {10.1086/317990},
    url = {},
    eprint = {https://academic.oup.com/poq/article-pdf/64/3/299/5258891/640299.pdf},
}

@article{Groves:2006,
author = {Groves, Robert M. and Heeringa, Steven G.},
title = {Responsive design for household surveys: tools for actively controlling survey errors and costs},
journal = {Journal of the Royal Statistical Society: Series A (Statistics in Society)},
volume = {169},
number = {3},
pages = {439-457},
keywords = {Multiphase sampling, Paradata, Propensity models, Responsive design, Survey non-response},
doi = {https://doi.org/10.1111/j.1467-985X.2006.00423.x},
url = {},
eprint = {https://rss.onlinelibrary.wiley.com/doi/pdf/10.1111/j.1467-985X.2006.00423.x},
abstract = {Summary.  Over the past few years surveys have expanded to new populations, have incorporated measurement of new and more complex substantive issues and have adopted new data collection tools. At the same time there has been a growing reluctance among many household populations to participate in surveys. These factors have combined to present survey designers and survey researchers with increased uncertainty about the performance of any given survey design at any particular point in time. This uncertainty has, in turn, challenged the survey practitioner's ability to control the cost of data collection and quality of resulting statistics. The development of computer-assisted methods for data collection has provided survey researchers with tools to capture a variety of process data (‘paradata’) that can be used to inform cost–quality trade-off decisions in realtime. The ability to monitor continually the streams of process data and survey data creates the opportunity to alter the design during the course of data collection to improve survey cost efficiency and to achieve more precise, less biased estimates. We label such surveys as ‘responsive designs’. The paper defines responsive design and uses examples to illustrate the responsive use of paradata to guide mid-survey decisions affecting the non-response, measurement and sampling variance properties of resulting statistics.},
year = {2006}
}

@article{Luiten:2013,
author = {Luiten, Annemieke and Schouten, Barry},
title = {Tailored fieldwork design to increase representative household survey response: an experiment in the Survey of Consumer Satisfaction},
journal = {Journal of the Royal Statistical Society: Series A (Statistics in Society)},
volume = {176},
number = {1},
pages = {169-189},
keywords = {Adaptive design, Non-response bias, Paradata, Representative response, R-indicators, Tailored design},
doi = {https://doi.org/10.1111/j.1467-985X.2012.01080.x},
url = {},
eprint = {https://rss.onlinelibrary.wiley.com/doi/pdf/10.1111/j.1467-985X.2012.01080.x},
abstract = {Summary.  We used a tailored survey design to obtain a more representative response. Paradata from previous consumer sentiments surveys and register information were used to stratify the sample into groups that differed in contact and co-operation propensity. We approached an experimental sample of 3000 households with a Web–mail–computer-assisted telephone interviewing sequential mixed mode strategy. The choice of initial mode and the subsequent computer-assisted telephone interviewing approach were tailored to the expected contact and co-operation propensities of the sample units. In the computer-assisted telephone interviewing follow-up of non-respondents, co-operation was manipulated by assigning specific interviewers to specific sample units. Contact was manipulated by timing, spacing and prioritizing calls. The tailored fieldwork strategy was successful in significantly increasing representativeness, while maintaining the level of response and costs. Representativeness was determined by R-indicators.},
year = {2013}
}

@article{Nishimura:2016,
author = {Nishimura, Raphael and Wagner, James and Elliott, Michael},
title = {Alternative Indicators for the Risk of Non-response Bias: A Simulation Study},
journal = {International Statistical Review},
volume = {84},
number = {1},
pages = {43-62},
keywords = {Bias, missing data, non-response, non-response indicators, survey data quality measures},
doi = {https://doi.org/10.1111/insr.12100},
url = {},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/insr.12100},
abstract = {Summary The growth of non-response rates for social science surveys has led to increased concern about the risk of non-response bias. Unfortunately, the non-response rate is a poor indicator of when non-response bias is likely to occur. We consider in this paper a set of alternative indicators. A large-scale simulation study is used to explore how each of these indicators performs in a variety of circumstances. Although, as expected, none of the indicators fully depict the impact of non-response in survey estimates, we discuss how they can be used when creating a plausible account of the risks for non-response bias for a survey. We also describe an interesting characteristic of the fraction of missing information that may be helpful in diagnosing not-missing-at-random mechanisms in certain situations.},
year = {2016}
}

@article{Rosen:2014,
	journal={Survey Practice},
	doi={10.29115/SP-2014-0001},
	number=1,
	title={Prioritizing Low Propensity Sample Members in a Survey: Implications for Nonresponse Bias},
	volume=7,
	author={Rosen, Jeffrey A and Murphy, Joe and Peytchev, Andy and Holder, Tommy and Dever, Jill and Herget, Debbie and Pratt, Daniel},
	date={2014-02-01},
	pages = {1-10},
	year=2014,
	month=2,
	day=1,
}

@article{Rosenblum:2019,
author = {Rosenblum, Michael and Miller, Peter and Reist, Benjamin and Stuart, Elizabeth A. and Thieme, Michael and Louis, Thomas A.},
title = {Adaptive design in surveys and clinical trials: similarities, differences and opportunities for cross-fertilization},
journal = {Journal of the Royal Statistical Society: Series A (Statistics in Society)},
volume = {182},
number = {3},
pages = {963-982},
keywords = {Adaptive design, Randomized trial, Sample survey},
doi = {https://doi.org/10.1111/rssa.12438},
url = {},
eprint = {https://rss.onlinelibrary.wiley.com/doi/pdf/10.1111/rssa.12438},
abstract = {Summary Adaptive designs involve preplanned rules for modifying an on-going study based on accruing data. We compare the goals and methods of adaptation for trials and surveys, identify similarities and differences, and make recommendations for what types of adaptive approaches from one domain have high potential to be useful in the other. For example, clinical trials could benefit from recently developed survey methods for monitoring which groups have low response rates and intervening to fix this. Clinical trials may also benefit from more formal identification of the target population, and from using paradata (contextual information collected before or during the collection of actual outcomes) to predict participant compliance and retention and then to intervene to improve these. Surveys could benefit from stopping rules based on information monitoring, applying techniques from sequential multiple-assignment randomized trial designs to improve response rates, prespecifying a formal adaptation protocol and including a data monitoring committee. We conclude with a discussion of the additional information, infrastructure and statistical analysis methods that are needed when conducting adaptive designs, as well as benefits and risks of adaptation.},
year = {2019}
}

@article{Sarndal:2010,
  title={The 2010 Morris Hansen Lecture. Dealing with survey nonresponse in data collection, in estimation},
  author={S{\"a}rndal, Carl-Erik},
  journal={Journal of Official Statistics},
  volume={27},
  number={1},
  pages={1},
  year={2011}
}

@article{Schouten:2013,
  title={Optimizing quality of response through adaptive survey designs},
  author={Schouten, Barry and Calinescu, Melania and Luiten, Annemieke},
  journal={Survey Methodology},
  volume={39},
  number={1},
  pages={29--58},
  year={2013}
}

@book{Schouten:2017:asd,
  title={Adaptive Survey Design},
  author={Schouten, B. and Peytchev, A. and Wagner, J.},
  isbn={9781498767880},
  lccn={2017011280},
  series={Chapman \& Hall/CRC Statistics in the Social and Behavioral Sciences},
  url={},
  year={2017},
  publisher={CRC Press},
  address={Boca Raton, FL}
}


@article{Schouten:2018:jssm,
    author = {Schouten, Barry and Mushkudiani, Nino and Shlomo, Natalie and Durrant, Gabi and Lundquist, Peter and Wagner, James},
    title = "{A Bayesian Analysis of Design Parameters in Survey Data Collection}",
    journal = {Journal of Survey Statistics and Methodology},
    volume = {6},
    number = {4},
    pages = {431-464},
    year = {2018},
    month = {07},
    abstract = "{In the design of surveys, a number of input parameters such as contact propensities, participation propensities, and costs per sample unit play a decisive role. In ongoing surveys, these survey design parameters are usually estimated from previous experience and updated gradually with new experience. In new surveys, these parameters are estimated from expert opinion and experience with similar surveys. Although survey institutes have fair expertise and experience, the postulation, estimation, and updating of survey design parameters is rarely done in a systematic way. This article presents a Bayesian framework to include and update prior knowledge and expert opinion about the parameters. This framework is set in the context of adaptive survey designs in which different population units may receive different treatment given quality and cost objectives. For this type of survey, the accuracy of design parameters becomes even more crucial to effective design decisions. The framework allows for a Bayesian analysis of the performance of a survey during data collection and in between waves of a survey. We demonstrate the utility of the Bayesian analysis using a simulation study based on the Dutch Health Survey.}",
    issn = {2325-0984},
    doi = {10.1093/jssam/smy012},
    eprint = {https://academic.oup.com/jssam/article-pdf/6/4/431/26348445/smy012.pdf},
}

@article{Schouten:2017:isr,
author = {Schouten, Barry and Shlomo, Natalie},
title = {Selecting Adaptive Survey Design Strata with Partial R-indicators},
journal = {International Statistical Review},
volume = {85},
number = {1},
pages = {143-163},
keywords = {Non-response, responsive survey design, representativeness, paradata},
doi = {https://doi.org/10.1111/insr.12159},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/insr.12159},
abstract = {Summary Recent survey literature shows an increasing interest in survey designs that adapt data collection to characteristics of the survey target population. Given a specified quality objective function, the designs attempt to find an optimal balance between quality and costs. Finding the optimal balance may not be straightforward as corresponding optimisation problems are often highly non-linear and non-convex. In this paper, we discuss how to choose strata in such designs and how to allocate these strata in a sequential design with two phases. We use partial R-indicators to build profiles of the data units where more or less attention is required in the data collection. In allocating cases, we look at two extremes: surveys that are run only once, or infrequent, and surveys that are run continuously. We demonstrate the impact of the sample size in a simulation study and provide an application to a real survey, the Dutch Crime Victimisation Survey.},
year = {2017}
}

@article{Schouten:2009,
  title={Indicators for the representativeness of survey response},
  author={Schouten, Barry and Cobben, Fannie and Bethlehem, Jelke and others},
  journal={Survey Methodology},
  volume={35},
  number={1},
  pages={101--113},
  year={2009}
}

@article{Vannieuwenhuyze:2012,
author = {Vannieuwenhuyze, Jorre T. A. and Loosveldt, Geert and Molenberghs, Geert},
title = {A Method to Evaluate Mode Effects on the Mean and Variance of a Continuous Variable in Mixed-Mode Surveys},
journal = {International Statistical Review},
volume = {80},
number = {2},
pages = {306-322},
keywords = {Mixed-mode, selection effect, measurement effect, mode effect, opinion about surveys},
doi = {https://doi.org/10.1111/j.1751-5823.2011.00167.x},
url = {},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1751-5823.2011.00167.x},
abstract = {Résumé Les enquêtes à mode de collecte mixte, dans lesquelles le choix du mode de réponse est laissé aux personnes interrogées, sont de plus en plus répandues. Cette possibilité du choix du mode de réponse peut induire une confusion entre deux sortes d'effets : les effets de sélection et les effets de mesure. L'estimation séparée de ces deux effets est impossible à partir des seules données d'une enquête à mode de collecte mixte. Dans cet article, nous montrons comment la même estimation devient possible à partir d'une comparaison entre les résultats d'une enquête à mode de collecte mixte et ceux d'un enquête à mode de collecte unique basées sur le même questionnaire. La méthode que nous proposons permet d'estimer l'effet du mode de réponse sur la moyenne et la variance d'une variable de réponse continue. Elle est illustrée par l'estimation des effets du mode de réponse sur six questions relatives aux opinions vis-à-vis des enquêtes.},
year = {2012}
}


@phdthesis{Wagner:2008,
  title={Adaptive Survey Design to Reduce Nonresponse Bias.},
  author={Wagner, James},
  year={2008},
  school={University of Michigan, Ann Arbor}
}

@inproceedings{Wagner:2014,
  title={Limiting the Risk of Nonresponse Bias by using Regression Diagnostics as a Guide to Data Collection},
  author={Wagner, James},
  booktitle={Joint Statistical Meetings},
  year={2014}
}

@article{Wagner:2019,
	journal={Survey Practice},
	doi={10.29115/SP-2018-0036},
	number=1,
	title={Estimation of Survey Cost Parameters Using Paradata},
	volume=12,
	author={Wagner, James},
	date={2019-05-28},
	year=2019,
	month=5,
	day=28,
}

@article{Wagner:2020,
author = {James Wagner and Brady T. West and Michael R. Elliott and Stephanie Coffey},
doi = {doi:10.2478/jos-2020-0043},
url = {},
title = {Comparing the Ability of Regression Modeling and Bayesian Additive Regression Trees to Predict Costs in a Responsive Survey Design Context},
journal = {Journal of Official Statistics},
number = {4},
volume = {36},
year = {2020},
pages = {907--931}
}





@DOITBYHAND{GEZO:2021,
	author = {CBS},
	date-added = {2021-01-03 5:22:09 PM +0100},
	date-modified = {2021-01-03 5:28:47 PM +0100},
	lastchecked = {January 3, 2021},
	title = {Health Survey as of 2014},
	url = {https://www.cbs.nl/en-gb/our-services/methods/surveys/korte-onderzoeksbeschrijvingen/health-survey-as-of-2014},
	year = {n.d.},
	NOTE = {\GEZO}
}


@manual{rpart,
    title = {rpart: Recursive Partitioning and Regression Trees},
    author = {Terry Therneau and Beth Atkinson},
    year = {2019},
    note = {R package version 4.1-15},
    url = {},
  }

@article{MCMCpack,
    title = {{MCMCpack}: Markov Chain Monte Carlo in {R}},
    author = {Andrew D. Martin and Kevin M. Quinn and Jong Hee Park},
    journal = {Journal of Statistical Software},
    year = {2011},
    volume = {42},
    number = {9},
    pages = {22},
    url = {},
}